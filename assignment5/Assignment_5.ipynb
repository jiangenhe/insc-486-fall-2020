{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiangenhe/insc-486-fall-2021/blob/main/assignment5/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbEshgPidmY0"
      },
      "source": [
        "# INSC486 Assignment 5 \n",
        "\n",
        "- Submit your Colab link through Canvas\n",
        "- This assignment is worth 90 points that accounts for 9% of your final grade.\n",
        "- **Code quality will be considered in grading.**\n",
        "\n",
        "Learning Objectives:\n",
        "- Logistic regression (Statistical analysis)\n",
        "- Build a supervised machine learning model (Classification)\n",
        "- Enhance your pandas skills"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-ZZtAf4UkYj"
      },
      "source": [
        "## Task 1: Poisson regression analysis (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wLpqvdiVuBt"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTUFaJ4BUti9"
      },
      "source": [
        "**Data**: https://raw.githubusercontent.com/jiangenhe/insc-486-fall-2021/main/assignment5/poisson_sim.csv\n",
        "\n",
        "**num_awards** is the outcome variable and indicates the number of awards earned by students at a high school in a year, **math** is a continuous predictor variable and represents students’ scores on their math final exam, and **prog** is a categorical predictor variable with three levels indicating the type of program in which the students were enrolled. It is coded as 1 = “General”, 2 = “Academic” and 3 = “Vocational”. Let’s start with loading the data and looking at some descriptive statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1p-TvkjWAjd"
      },
      "source": [
        "### 1. Perform a Poisson regression analaysis to predict the number of awards earned by students at one high school. \n",
        "\n",
        "Predictors of the number of awards earned include the type of program in which the student was enrolled (e.g., vocational, general or academic) and the score on their final exam in math."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvIC7qMYXJEy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULCDlmBaXJgk"
      },
      "source": [
        "### 2. Interpret the regression results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX8UwUmrXRg-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mGqoJmXeO-"
      },
      "source": [
        "## Task 2: Build a classifier (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK4T3KcZYqPo"
      },
      "source": [
        "For this task, you will be using the Breast Cancer Wisconsin (Diagnostic) Database to create a classifier that can help diagnose patients. First, read through the description of the dataset (below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrrDRIqQYpeu",
        "outputId": "51fa57fe-cf93-4388-ee8a-1e18ac980afb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "print(cancer.DESCR) # Print the data set description"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _breast_cancer_dataset:\n",
            "\n",
            "Breast cancer wisconsin (diagnostic) dataset\n",
            "--------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 569\n",
            "\n",
            "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
            "\n",
            "    :Attribute Information:\n",
            "        - radius (mean of distances from center to points on the perimeter)\n",
            "        - texture (standard deviation of gray-scale values)\n",
            "        - perimeter\n",
            "        - area\n",
            "        - smoothness (local variation in radius lengths)\n",
            "        - compactness (perimeter^2 / area - 1.0)\n",
            "        - concavity (severity of concave portions of the contour)\n",
            "        - concave points (number of concave portions of the contour)\n",
            "        - symmetry\n",
            "        - fractal dimension (\"coastline approximation\" - 1)\n",
            "\n",
            "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
            "        worst/largest values) of these features were computed for each image,\n",
            "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
            "        10 is Radius SE, field 20 is Worst Radius.\n",
            "\n",
            "        - class:\n",
            "                - WDBC-Malignant\n",
            "                - WDBC-Benign\n",
            "\n",
            "    :Summary Statistics:\n",
            "\n",
            "    ===================================== ====== ======\n",
            "                                           Min    Max\n",
            "    ===================================== ====== ======\n",
            "    radius (mean):                        6.981  28.11\n",
            "    texture (mean):                       9.71   39.28\n",
            "    perimeter (mean):                     43.79  188.5\n",
            "    area (mean):                          143.5  2501.0\n",
            "    smoothness (mean):                    0.053  0.163\n",
            "    compactness (mean):                   0.019  0.345\n",
            "    concavity (mean):                     0.0    0.427\n",
            "    concave points (mean):                0.0    0.201\n",
            "    symmetry (mean):                      0.106  0.304\n",
            "    fractal dimension (mean):             0.05   0.097\n",
            "    radius (standard error):              0.112  2.873\n",
            "    texture (standard error):             0.36   4.885\n",
            "    perimeter (standard error):           0.757  21.98\n",
            "    area (standard error):                6.802  542.2\n",
            "    smoothness (standard error):          0.002  0.031\n",
            "    compactness (standard error):         0.002  0.135\n",
            "    concavity (standard error):           0.0    0.396\n",
            "    concave points (standard error):      0.0    0.053\n",
            "    symmetry (standard error):            0.008  0.079\n",
            "    fractal dimension (standard error):   0.001  0.03\n",
            "    radius (worst):                       7.93   36.04\n",
            "    texture (worst):                      12.02  49.54\n",
            "    perimeter (worst):                    50.41  251.2\n",
            "    area (worst):                         185.2  4254.0\n",
            "    smoothness (worst):                   0.071  0.223\n",
            "    compactness (worst):                  0.027  1.058\n",
            "    concavity (worst):                    0.0    1.252\n",
            "    concave points (worst):               0.0    0.291\n",
            "    symmetry (worst):                     0.156  0.664\n",
            "    fractal dimension (worst):            0.055  0.208\n",
            "    ===================================== ====== ======\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
            "\n",
            "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
            "\n",
            "    :Donor: Nick Street\n",
            "\n",
            "    :Date: November, 1995\n",
            "\n",
            "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
            "https://goo.gl/U2Uwz2\n",
            "\n",
            "Features are computed from a digitized image of a fine needle\n",
            "aspirate (FNA) of a breast mass.  They describe\n",
            "characteristics of the cell nuclei present in the image.\n",
            "\n",
            "Separating plane described above was obtained using\n",
            "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
            "Construction Via Linear Programming.\" Proceedings of the 4th\n",
            "Midwest Artificial Intelligence and Cognitive Science Society,\n",
            "pp. 97-101, 1992], a classification method which uses linear\n",
            "programming to construct a decision tree.  Relevant features\n",
            "were selected using an exhaustive search in the space of 1-4\n",
            "features and 1-3 separating planes.\n",
            "\n",
            "The actual linear program used to obtain the separating plane\n",
            "in the 3-dimensional space is that described in:\n",
            "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
            "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
            "Optimization Methods and Software 1, 1992, 23-34].\n",
            "\n",
            "This database is also available through the UW CS ftp server:\n",
            "\n",
            "ftp ftp.cs.wisc.edu\n",
            "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
            "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
            "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
            "     San Jose, CA, 1993.\n",
            "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
            "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
            "     July-August 1995.\n",
            "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
            "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
            "     163-171.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYjKWReZjNMI"
      },
      "source": [
        "The object returned by load_breast_cancer() is a scikit-learn Bunch object, which is similar to a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlwXVcWbjMAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ccb5a1d-8238-43d2-91f4-0667573e72e8"
      },
      "source": [
        "cancer.keys()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhwPuGqXjoU1"
      },
      "source": [
        "### 1. Convert the sklearn.dataset `cancer` to a `(569, 31)` DataFrame `cancerdf`. (Code for this question has been provided)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJGc0vqzkG1M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "9cf87512-c2d9-4a70-94c1-91ec70a637cb"
      },
      "source": [
        "cancerdf = pd.concat([pd.DataFrame(cancer.data ), pd.DataFrame(cancer.target)], axis=1)\n",
        "cancerdf.describe()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.127292</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.062798</td>\n",
              "      <td>0.405172</td>\n",
              "      <td>1.216853</td>\n",
              "      <td>2.866059</td>\n",
              "      <td>40.337079</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.011796</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>0.003795</td>\n",
              "      <td>16.269190</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.083946</td>\n",
              "      <td>0.627417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.524049</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.277313</td>\n",
              "      <td>0.551648</td>\n",
              "      <td>2.021855</td>\n",
              "      <td>45.491006</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.030186</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>4.833242</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.018061</td>\n",
              "      <td>0.483918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.049960</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.757000</td>\n",
              "      <td>6.802000</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>7.930000</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>11.700000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.833900</td>\n",
              "      <td>1.606000</td>\n",
              "      <td>17.850000</td>\n",
              "      <td>0.005169</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.007638</td>\n",
              "      <td>0.015160</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>13.010000</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.071460</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.370000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.324200</td>\n",
              "      <td>1.108000</td>\n",
              "      <td>2.287000</td>\n",
              "      <td>24.530000</td>\n",
              "      <td>0.006380</td>\n",
              "      <td>0.020450</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>0.018730</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>14.970000</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.080040</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>1.474000</td>\n",
              "      <td>3.357000</td>\n",
              "      <td>45.190000</td>\n",
              "      <td>0.008146</td>\n",
              "      <td>0.032450</td>\n",
              "      <td>0.042050</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.023480</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>18.790000</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.092080</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.097440</td>\n",
              "      <td>2.873000</td>\n",
              "      <td>4.885000</td>\n",
              "      <td>21.980000</td>\n",
              "      <td>542.200000</td>\n",
              "      <td>0.031130</td>\n",
              "      <td>0.135400</td>\n",
              "      <td>0.396000</td>\n",
              "      <td>0.052790</td>\n",
              "      <td>0.078950</td>\n",
              "      <td>0.029840</td>\n",
              "      <td>36.040000</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               0           1           2   ...          28          29          0 \n",
              "count  569.000000  569.000000  569.000000  ...  569.000000  569.000000  569.000000\n",
              "mean    14.127292   19.289649   91.969033  ...    0.290076    0.083946    0.627417\n",
              "std      3.524049    4.301036   24.298981  ...    0.061867    0.018061    0.483918\n",
              "min      6.981000    9.710000   43.790000  ...    0.156500    0.055040    0.000000\n",
              "25%     11.700000   16.170000   75.170000  ...    0.250400    0.071460    0.000000\n",
              "50%     13.370000   18.840000   86.240000  ...    0.282200    0.080040    1.000000\n",
              "75%     15.780000   21.800000  104.100000  ...    0.317900    0.092080    1.000000\n",
              "max     28.110000   39.280000  188.500000  ...    0.663800    0.207500    1.000000\n",
              "\n",
              "[8 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzX5ZaxekHN-"
      },
      "source": [
        "### 2. What is the class distribution? (i.e. how many instances of `malignant` (encoded 0) and how many `benign` (encoded 1)?)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0LsWRMCkqL3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1VMJ005kqqp"
      },
      "source": [
        "### 3. Split the DataFrame into `X` (the data) and `y` (the labels).\n",
        "\n",
        "* `X`*, a pandas DataFrame, has shape* `(569, 30)`\n",
        "* `y`*, a pandas Series, has shape* `(569,)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qDbdDL0k2Y9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i1u768Rk24N"
      },
      "source": [
        "### 4. Using `train_test_split`, split `X` and `y` into training and test sets `X_train`, `X_test`, `y_train`, and `y_test`.\n",
        "\n",
        "* `X_train` *has shape* `(426, 30)`\n",
        "* `X_test` *has shape* `(143, 30)`\n",
        "* `y_train` *has shape* `(426,)`\n",
        "* `y_test` *has shape* `(143,)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtAAU2prlm0z"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pn5kQZalqX3"
      },
      "source": [
        "### 5. Using KNeighborsClassifier, fit a k-nearest neighbors (knn) classifier with `X_train`, `y_train` and using one nearest neighbor (`n_neighbors = 1`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TvmECMjl1qB"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMqdSoGomc7r"
      },
      "source": [
        "### 6. Using your knn classifier, predict the class label using the mean value for each feature.\n",
        "\n",
        "Hint: You can use `cancerdf.mean()[:-1].values.reshape(1, -1)` which gets the mean value for each feature, ignores the target column, and reshapes the data from 1 dimension to 2 (necessary for the precict method of KNeighborsClassifier)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "proiNBw3m0qp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ9FuTtpnEno"
      },
      "source": [
        "### 7. Use the knn classifer your created for Question 5 to predict the class labels for the test set `X_text`.\n",
        "\n",
        "*The result should be a numpy array with shape `(143,)` and values either `0.0` or `1.0`.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n1Yw5UUnut_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm8E9kdbnvEe"
      },
      "source": [
        "### 8. Find the score (mean accuracy) of your knn classifier using `X_test` and `y_test`.\n",
        "\n",
        "*The calculated score should be a float between 0 and 1*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq7C7novn7dS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoL-s8oXoB3w"
      },
      "source": [
        "### 9. Read and run the code below to visualize the differet predicition scores between training and test sets, as well as malignant and benign cells.\n",
        "\n",
        "*Correct variable names if they are different from yours, including `X_train`, `y_train`, `X_test`, `y_test` from question 4 and `knn` from question 5.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3RAyEo4oTmc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib notebook\n",
        "\n",
        "# Find the training and testing accuracies by target value (i.e. malignant, benign)\n",
        "mal_train_X = X_train[y_train==0]\n",
        "mal_train_y = y_train[y_train==0]\n",
        "ben_train_X = X_train[y_train==1]\n",
        "ben_train_y = y_train[y_train==1]\n",
        "\n",
        "mal_test_X = X_test[y_test==0]\n",
        "mal_test_y = y_test[y_test==0]\n",
        "ben_test_X = X_test[y_test==1]\n",
        "ben_test_y = y_test[y_test==1]\n",
        "\n",
        "\n",
        "scores = [knn.score(mal_train_X, mal_train_y), knn.score(ben_train_X, ben_train_y), knn.score(mal_test_X, mal_test_y), knn.score(ben_test_X, ben_test_y)]\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot the scores as a bar chart\n",
        "bars = plt.bar(np.arange(4), scores, color=['#4c72b0','#4c72b0','#55a868','#55a868'])\n",
        "\n",
        "# directly label the score onto the bars\n",
        "for bar in bars:\n",
        "  height = bar.get_height()\n",
        "  plt.gca().text(bar.get_x() + bar.get_width()/2, height*.90, '{0:.{1}f}'.format(height, 2), ha='center', color='w', fontsize=11)\n",
        "\n",
        "# remove all the ticks (both axes), and tick labels on the Y axis\n",
        "plt.tick_params(top='off', bottom='off', left='off', right='off', labelleft='off', labelbottom='on')\n",
        "\n",
        "# remove the frame of the chart\n",
        "for spine in plt.gca().spines.values():\n",
        "  spine.set_visible(False)\n",
        "\n",
        "plt.xticks([0,1,2,3], ['Malignant\\nTraining', 'Benign\\nTraining', 'Malignant\\nTest', 'Benign\\nTest'], alpha=0.8);\n",
        "plt.title('Training and Test Accuracies for Malignant and Benign Cells', alpha=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzmMScd1sh_w"
      },
      "source": [
        "## Task 3: Classification Playspace (40%)\n",
        "\n",
        "For this task, you will build classifiers to classify an iris species as either (virginica, setosa, or versicolor) for the iris flower dataset. You will use different machine learning algorithm to build the classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oftnB-oOpesk"
      },
      "source": [
        "### 0. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvlM9euBn85j"
      },
      "source": [
        "import seaborn as sns\n",
        "iris = sns.load_dataset(\"iris\")\n",
        "\n",
        "# X = feature values, all the columns except the last column\n",
        "X = iris.iloc[:, :-1]\n",
        "\n",
        "# y = target values, last column of the data frame\n",
        "y = iris.iloc[:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA70Sl8Mrbar"
      },
      "source": [
        "### 1. Build a Basic Understanding of the dataset\n",
        "\n",
        "Use head(), describe() and any other pandas features to explore the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qivOFPALr9J4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG80-gyCrf3w"
      },
      "source": [
        "### 2. Visualize the Dataset\n",
        "Read the code and run it. You don't need to make any change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVY0m-JjsT6Q"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Species')\n",
        "\n",
        "pltX = iris.loc[:, 'sepal_length']\n",
        "pltY = iris.loc[:,'species']\n",
        "plt.scatter(pltX, pltY, color='blue', label='sepal_length')\n",
        "\n",
        "pltX = iris.loc[:, 'sepal_width']\n",
        "pltY = iris.loc[:,'species']\n",
        "plt.scatter(pltX, pltY, color='green', label='sepal_width')\n",
        "\n",
        "pltX = iris.loc[:, 'petal_length']\n",
        "pltY = iris.loc[:,'species']\n",
        "plt.scatter(pltX, pltY, color='red', label='petal_length')\n",
        "\n",
        "pltX = iris.loc[:, 'petal_width']\n",
        "pltY = iris.loc[:,'species']\n",
        "plt.scatter(pltX, pltY, color='black', label='petal_width')\n",
        "\n",
        "plt.legend(loc=4, prop={'size':8})\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBh1We45u88-"
      },
      "source": [
        "**For question 3 - 6, you are feel to set hyperparameters that you think is reasonable.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT_b-HSGtGrK"
      },
      "source": [
        "### 3. Use kNN to build a classifier, including train/test data split, fit, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nOziStQtqIO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5R0t29rt2p8"
      },
      "source": [
        "### 4. Use logistic model to build a classifier, including train/test data split, fit, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjwSGi2St-L5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRn32_H4t_pZ"
      },
      "source": [
        "### 5. Use SVM to build a classifier, including train/test data split, fit, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wemFEkeuWOx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpigAbATuYo0"
      },
      "source": [
        "### 6. Use Decision Tree Model to build a classifier, including train/test data split, fit, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uSEx_Dgujzd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BhhUIKZuvB7"
      },
      "source": [
        "### 7. Select one of the models to set different hyperparameters to compare the performace difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XotBM2VXvrXq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG0_ENtKuu-H"
      },
      "source": [
        "### 8. Select one of the models to apply cross validataion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbOcIO41v9Wt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}